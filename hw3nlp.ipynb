{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1l92-FcSM_8fDbazgdqnmoABGX39Ughb1",
      "authorship_tag": "ABX9TyMR5RFl7PijcX+ve0kjMtWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzandie/colab/blob/main/hw3nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9oGwwWEKOd8l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/My Drive/sentiment140.csv'  # path file in Google Drive\n",
        "data = pd.read_csv(file_path, encoding='latin-1')\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TmGBWgsSDEG",
        "outputId": "fd429d48-32fb-48dc-f907-db90ceab7e8d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
            "1  is upset that he can't update his Facebook by ...   \n",
            "2  @Kenichan I dived many times for the ball. Man...   \n",
            "3    my whole body feels itchy and like its on fire    \n",
            "4  @nationwideclass no, it's not behaving at all....   \n",
            "\n",
            "                           date             user  sentiment     query  \n",
            "0  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
            "1  Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
            "2  Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
            "3  Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
            "4  Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# تبدیل برچسب‌ها\n",
        "data['sentiment'] = data['sentiment'].replace({4: 1, 0: 0})"
      ],
      "metadata": {
        "id": "L8JlXz5QYsCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# نمایش چند سطر از داده‌های تبدیل شده\n",
        "print(data.head())\n",
        "print(\"------------------------------------------------\")\n",
        "print(data.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDoQ4wORZ2f4",
        "outputId": "1f34e23f-7e82-4977-985e-6daa5cd54203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
            "1  is upset that he can't update his Facebook by ...   \n",
            "2  @Kenichan I dived many times for the ball. Man...   \n",
            "3    my whole body feels itchy and like its on fire    \n",
            "4  @nationwideclass no, it's not behaving at all....   \n",
            "\n",
            "                           date             user  sentiment     query  \n",
            "0  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
            "1  Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
            "2  Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
            "3  Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
            "4  Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n",
            "------------------------------------------------\n",
            "                                                      text  \\\n",
            "1599995  Just woke up. Having no school is the best fee...   \n",
            "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
            "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
            "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
            "1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
            "\n",
            "                                 date             user  sentiment     query  \n",
            "1599995  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028          1  NO_QUERY  \n",
            "1599996  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards          1  NO_QUERY  \n",
            "1599997  Tue Jun 16 08:40:49 PDT 2009           bpbabe          1  NO_QUERY  \n",
            "1599998  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz          1  NO_QUERY  \n",
            "1599999  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris          1  NO_QUERY  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_tokens(text):\n",
        "    text = re.sub(r'http\\S+', 'URL', text)\n",
        "    text = re.sub(r'@\\w+', 'MENTION', text)\n",
        "    text = re.sub(r'#\\w+', 'HASHTAG', text)\n",
        "    return text\n",
        "\n",
        "data['text'] = data['text'].apply(replace_tokens)\n",
        "print(data.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxF5XLovali1",
        "outputId": "38b6182e-c486-4c9f-9439-1b2c877579aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  \\\n",
            "0   MENTION URL - Awww, that's a bummer.  You shou...   \n",
            "1   is upset that he can't update his Facebook by ...   \n",
            "2   MENTION I dived many times for the ball. Manag...   \n",
            "3     my whole body feels itchy and like its on fire    \n",
            "4   MENTION no, it's not behaving at all. i'm mad....   \n",
            "5                         MENTION not the whole crew    \n",
            "6                                         Need a hug    \n",
            "7   MENTION hey  long time no see! Yes.. Rains a b...   \n",
            "8                   MENTION nope they didn't have it    \n",
            "9                             MENTION que me muera ?    \n",
            "10        spring break in plain city... it's snowing    \n",
            "11                         I just re-pierced my ears    \n",
            "12  MENTION I couldn't bear to watch it.  And I th...   \n",
            "13  MENTION It it counts, idk why I did either. yo...   \n",
            "14  MENTION i would've been the first, but i didn'...   \n",
            "15  MENTION I wish I got to watch it with you!! I ...   \n",
            "16  Hollis' death scene will hurt me severely to w...   \n",
            "17                               about to file taxes    \n",
            "18  MENTION ahh ive always wanted to see rent  lov...   \n",
            "19  MENTION Oh dear. Were you drinking out of the ...   \n",
            "\n",
            "                            date             user  sentiment     query  \n",
            "0   Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
            "1   Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
            "2   Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
            "3   Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
            "4   Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n",
            "5   Mon Apr 06 22:20:00 PDT 2009         joy_wolf          0  NO_QUERY  \n",
            "6   Mon Apr 06 22:20:03 PDT 2009          mybirch          0  NO_QUERY  \n",
            "7   Mon Apr 06 22:20:03 PDT 2009             coZZ          0  NO_QUERY  \n",
            "8   Mon Apr 06 22:20:05 PDT 2009  2Hood4Hollywood          0  NO_QUERY  \n",
            "9   Mon Apr 06 22:20:09 PDT 2009          mimismo          0  NO_QUERY  \n",
            "10  Mon Apr 06 22:20:16 PDT 2009   erinx3leannexo          0  NO_QUERY  \n",
            "11  Mon Apr 06 22:20:17 PDT 2009     pardonlauren          0  NO_QUERY  \n",
            "12  Mon Apr 06 22:20:19 PDT 2009             TLeC          0  NO_QUERY  \n",
            "13  Mon Apr 06 22:20:19 PDT 2009  robrobbierobert          0  NO_QUERY  \n",
            "14  Mon Apr 06 22:20:20 PDT 2009      bayofwolves          0  NO_QUERY  \n",
            "15  Mon Apr 06 22:20:20 PDT 2009       HairByJess          0  NO_QUERY  \n",
            "16  Mon Apr 06 22:20:22 PDT 2009   lovesongwriter          0  NO_QUERY  \n",
            "17  Mon Apr 06 22:20:25 PDT 2009         armotley          0  NO_QUERY  \n",
            "18  Mon Apr 06 22:20:31 PDT 2009       starkissed          0  NO_QUERY  \n",
            "19  Mon Apr 06 22:20:34 PDT 2009        gi_gi_bee          0  NO_QUERY  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "data['text'] = data['text'].apply(remove_punctuation)\n",
        "print (data.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNw3guy8bpSI",
        "outputId": "9d63545b-fc08-482f-9cb6-1b43bc155925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  \\\n",
            "0   MENTION URL  Awww thats a bummer  You shoulda ...   \n",
            "1   is upset that he cant update his Facebook by t...   \n",
            "2   MENTION I dived many times for the ball Manage...   \n",
            "3     my whole body feels itchy and like its on fire    \n",
            "4   MENTION no its not behaving at all im mad why ...   \n",
            "5                         MENTION not the whole crew    \n",
            "6                                         Need a hug    \n",
            "7   MENTION hey  long time no see Yes Rains a bit ...   \n",
            "8                    MENTION nope they didnt have it    \n",
            "9                              MENTION que me muera     \n",
            "10            spring break in plain city its snowing    \n",
            "11                          I just repierced my ears    \n",
            "12  MENTION I couldnt bear to watch it  And I thou...   \n",
            "13  MENTION It it counts idk why I did either you ...   \n",
            "14  MENTION i wouldve been the first but i didnt h...   \n",
            "15  MENTION I wish I got to watch it with you I mi...   \n",
            "16  Hollis death scene will hurt me severely to wa...   \n",
            "17                               about to file taxes    \n",
            "18  MENTION ahh ive always wanted to see rent  lov...   \n",
            "19  MENTION Oh dear Were you drinking out of the f...   \n",
            "\n",
            "                            date             user  sentiment     query  \n",
            "0   Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
            "1   Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
            "2   Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
            "3   Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
            "4   Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n",
            "5   Mon Apr 06 22:20:00 PDT 2009         joy_wolf          0  NO_QUERY  \n",
            "6   Mon Apr 06 22:20:03 PDT 2009          mybirch          0  NO_QUERY  \n",
            "7   Mon Apr 06 22:20:03 PDT 2009             coZZ          0  NO_QUERY  \n",
            "8   Mon Apr 06 22:20:05 PDT 2009  2Hood4Hollywood          0  NO_QUERY  \n",
            "9   Mon Apr 06 22:20:09 PDT 2009          mimismo          0  NO_QUERY  \n",
            "10  Mon Apr 06 22:20:16 PDT 2009   erinx3leannexo          0  NO_QUERY  \n",
            "11  Mon Apr 06 22:20:17 PDT 2009     pardonlauren          0  NO_QUERY  \n",
            "12  Mon Apr 06 22:20:19 PDT 2009             TLeC          0  NO_QUERY  \n",
            "13  Mon Apr 06 22:20:19 PDT 2009  robrobbierobert          0  NO_QUERY  \n",
            "14  Mon Apr 06 22:20:20 PDT 2009      bayofwolves          0  NO_QUERY  \n",
            "15  Mon Apr 06 22:20:20 PDT 2009       HairByJess          0  NO_QUERY  \n",
            "16  Mon Apr 06 22:20:22 PDT 2009   lovesongwriter          0  NO_QUERY  \n",
            "17  Mon Apr 06 22:20:25 PDT 2009         armotley          0  NO_QUERY  \n",
            "18  Mon Apr 06 22:20:31 PDT 2009       starkissed          0  NO_QUERY  \n",
            "19  Mon Apr 06 22:20:34 PDT 2009        gi_gi_bee          0  NO_QUERY  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "data['tokens'] = data['text'].apply(tokenize)\n",
        "print (data.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDzOVquWgI7j",
        "outputId": "8b640748-5a0e-4969-a103-5ba97a93b705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  \\\n",
            "0   MENTION URL  Awww thats a bummer  You shoulda ...   \n",
            "1   is upset that he cant update his Facebook by t...   \n",
            "2   MENTION I dived many times for the ball Manage...   \n",
            "3     my whole body feels itchy and like its on fire    \n",
            "4   MENTION no its not behaving at all im mad why ...   \n",
            "5                         MENTION not the whole crew    \n",
            "6                                         Need a hug    \n",
            "7   MENTION hey  long time no see Yes Rains a bit ...   \n",
            "8                    MENTION nope they didnt have it    \n",
            "9                              MENTION que me muera     \n",
            "10            spring break in plain city its snowing    \n",
            "11                          I just repierced my ears    \n",
            "12  MENTION I couldnt bear to watch it  And I thou...   \n",
            "13  MENTION It it counts idk why I did either you ...   \n",
            "14  MENTION i wouldve been the first but i didnt h...   \n",
            "15  MENTION I wish I got to watch it with you I mi...   \n",
            "16  Hollis death scene will hurt me severely to wa...   \n",
            "17                               about to file taxes    \n",
            "18  MENTION ahh ive always wanted to see rent  lov...   \n",
            "19  MENTION Oh dear Were you drinking out of the f...   \n",
            "\n",
            "                            date             user  sentiment     query  \\\n",
            "0   Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY   \n",
            "1   Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY   \n",
            "2   Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY   \n",
            "3   Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY   \n",
            "4   Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY   \n",
            "5   Mon Apr 06 22:20:00 PDT 2009         joy_wolf          0  NO_QUERY   \n",
            "6   Mon Apr 06 22:20:03 PDT 2009          mybirch          0  NO_QUERY   \n",
            "7   Mon Apr 06 22:20:03 PDT 2009             coZZ          0  NO_QUERY   \n",
            "8   Mon Apr 06 22:20:05 PDT 2009  2Hood4Hollywood          0  NO_QUERY   \n",
            "9   Mon Apr 06 22:20:09 PDT 2009          mimismo          0  NO_QUERY   \n",
            "10  Mon Apr 06 22:20:16 PDT 2009   erinx3leannexo          0  NO_QUERY   \n",
            "11  Mon Apr 06 22:20:17 PDT 2009     pardonlauren          0  NO_QUERY   \n",
            "12  Mon Apr 06 22:20:19 PDT 2009             TLeC          0  NO_QUERY   \n",
            "13  Mon Apr 06 22:20:19 PDT 2009  robrobbierobert          0  NO_QUERY   \n",
            "14  Mon Apr 06 22:20:20 PDT 2009      bayofwolves          0  NO_QUERY   \n",
            "15  Mon Apr 06 22:20:20 PDT 2009       HairByJess          0  NO_QUERY   \n",
            "16  Mon Apr 06 22:20:22 PDT 2009   lovesongwriter          0  NO_QUERY   \n",
            "17  Mon Apr 06 22:20:25 PDT 2009         armotley          0  NO_QUERY   \n",
            "18  Mon Apr 06 22:20:31 PDT 2009       starkissed          0  NO_QUERY   \n",
            "19  Mon Apr 06 22:20:34 PDT 2009        gi_gi_bee          0  NO_QUERY   \n",
            "\n",
            "                                               tokens  \n",
            "0   [MENTION, URL, Awww, thats, a, bummer, You, sh...  \n",
            "1   [is, upset, that, he, cant, update, his, Faceb...  \n",
            "2   [MENTION, I, dived, many, times, for, the, bal...  \n",
            "3   [my, whole, body, feels, itchy, and, like, its...  \n",
            "4   [MENTION, no, its, not, behaving, at, all, im,...  \n",
            "5                    [MENTION, not, the, whole, crew]  \n",
            "6                                      [Need, a, hug]  \n",
            "7   [MENTION, hey, long, time, no, see, Yes, Rains...  \n",
            "8              [MENTION, nope, they, didnt, have, it]  \n",
            "9                           [MENTION, que, me, muera]  \n",
            "10     [spring, break, in, plain, city, its, snowing]  \n",
            "11                     [I, just, repierced, my, ears]  \n",
            "12  [MENTION, I, couldnt, bear, to, watch, it, And...  \n",
            "13  [MENTION, It, it, counts, idk, why, I, did, ei...  \n",
            "14  [MENTION, i, wouldve, been, the, first, but, i...  \n",
            "15  [MENTION, I, wish, I, got, to, watch, it, with...  \n",
            "16  [Hollis, death, scene, will, hurt, me, severel...  \n",
            "17                           [about, to, file, taxes]  \n",
            "18  [MENTION, ahh, ive, always, wanted, to, see, r...  \n",
            "19  [MENTION, Oh, dear, Were, you, drinking, out, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "data['tokens'] = data['tokens'].apply(lemmatize_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttbOdiJ5lXf3",
        "outputId": "165596a3-af89-407f-8b5c-41008a1f884b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-xGo1mBl7SO",
        "outputId": "4c5c73b5-21ab-4d8b-bc6d-b4a2f9546fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  \\\n",
            "0   MENTION URL  Awww thats a bummer  You shoulda ...   \n",
            "1   is upset that he cant update his Facebook by t...   \n",
            "2   MENTION I dived many times for the ball Manage...   \n",
            "3     my whole body feels itchy and like its on fire    \n",
            "4   MENTION no its not behaving at all im mad why ...   \n",
            "5                         MENTION not the whole crew    \n",
            "6                                         Need a hug    \n",
            "7   MENTION hey  long time no see Yes Rains a bit ...   \n",
            "8                    MENTION nope they didnt have it    \n",
            "9                              MENTION que me muera     \n",
            "10            spring break in plain city its snowing    \n",
            "11                          I just repierced my ears    \n",
            "12  MENTION I couldnt bear to watch it  And I thou...   \n",
            "13  MENTION It it counts idk why I did either you ...   \n",
            "14  MENTION i wouldve been the first but i didnt h...   \n",
            "15  MENTION I wish I got to watch it with you I mi...   \n",
            "16  Hollis death scene will hurt me severely to wa...   \n",
            "17                               about to file taxes    \n",
            "18  MENTION ahh ive always wanted to see rent  lov...   \n",
            "19  MENTION Oh dear Were you drinking out of the f...   \n",
            "\n",
            "                            date             user  sentiment     query  \\\n",
            "0   Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY   \n",
            "1   Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY   \n",
            "2   Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY   \n",
            "3   Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY   \n",
            "4   Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY   \n",
            "5   Mon Apr 06 22:20:00 PDT 2009         joy_wolf          0  NO_QUERY   \n",
            "6   Mon Apr 06 22:20:03 PDT 2009          mybirch          0  NO_QUERY   \n",
            "7   Mon Apr 06 22:20:03 PDT 2009             coZZ          0  NO_QUERY   \n",
            "8   Mon Apr 06 22:20:05 PDT 2009  2Hood4Hollywood          0  NO_QUERY   \n",
            "9   Mon Apr 06 22:20:09 PDT 2009          mimismo          0  NO_QUERY   \n",
            "10  Mon Apr 06 22:20:16 PDT 2009   erinx3leannexo          0  NO_QUERY   \n",
            "11  Mon Apr 06 22:20:17 PDT 2009     pardonlauren          0  NO_QUERY   \n",
            "12  Mon Apr 06 22:20:19 PDT 2009             TLeC          0  NO_QUERY   \n",
            "13  Mon Apr 06 22:20:19 PDT 2009  robrobbierobert          0  NO_QUERY   \n",
            "14  Mon Apr 06 22:20:20 PDT 2009      bayofwolves          0  NO_QUERY   \n",
            "15  Mon Apr 06 22:20:20 PDT 2009       HairByJess          0  NO_QUERY   \n",
            "16  Mon Apr 06 22:20:22 PDT 2009   lovesongwriter          0  NO_QUERY   \n",
            "17  Mon Apr 06 22:20:25 PDT 2009         armotley          0  NO_QUERY   \n",
            "18  Mon Apr 06 22:20:31 PDT 2009       starkissed          0  NO_QUERY   \n",
            "19  Mon Apr 06 22:20:34 PDT 2009        gi_gi_bee          0  NO_QUERY   \n",
            "\n",
            "                                               tokens  \n",
            "0   [MENTION, URL, Awww, thats, a, bummer, You, sh...  \n",
            "1   [is, upset, that, he, cant, update, his, Faceb...  \n",
            "2   [MENTION, I, dived, many, time, for, the, ball...  \n",
            "3   [my, whole, body, feel, itchy, and, like, it, ...  \n",
            "4   [MENTION, no, it, not, behaving, at, all, im, ...  \n",
            "5                    [MENTION, not, the, whole, crew]  \n",
            "6                                      [Need, a, hug]  \n",
            "7   [MENTION, hey, long, time, no, see, Yes, Rains...  \n",
            "8              [MENTION, nope, they, didnt, have, it]  \n",
            "9                           [MENTION, que, me, muera]  \n",
            "10      [spring, break, in, plain, city, it, snowing]  \n",
            "11                      [I, just, repierced, my, ear]  \n",
            "12  [MENTION, I, couldnt, bear, to, watch, it, And...  \n",
            "13  [MENTION, It, it, count, idk, why, I, did, eit...  \n",
            "14  [MENTION, i, wouldve, been, the, first, but, i...  \n",
            "15  [MENTION, I, wish, I, got, to, watch, it, with...  \n",
            "16  [Hollis, death, scene, will, hurt, me, severel...  \n",
            "17                             [about, to, file, tax]  \n",
            "18  [MENTION, ahh, ive, always, wanted, to, see, r...  \n",
            "19  [MENTION, Oh, dear, Were, you, drinking, out, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data['tokens'], data['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# تبدیل لیست به آرایه‌های NumPy\n",
        "train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n"
      ],
      "metadata": {
        "id": "CbIibuXnmatS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# نمایش اطلاعات مجموعه داده‌ها\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Testing data shape: {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WZA0SD7n3Pt",
        "outputId": "a5218f96-8224-41e7-b935-7d0b03fd16be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (1280000,)\n",
            "Testing data shape: (320000,)\n"
          ]
        }
      ]
    }
  ]
}